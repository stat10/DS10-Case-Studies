{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stat10/DS10-Case-Studies/blob/master/Democratic_Primary_Case_Study_3_6_2020.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWfswokfWdr2"
   },
   "source": [
    "# Predicting the 2020 Democratic Primary Election\n",
    "\n",
    "## Case Overview: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The 2020 Democratic Primary Election will be a pivotal moment in American (and world) history. As such, this is an interesting and significant race to analyze and better understand. In this case study, we will explore the Democratic primary election from a variety of angles with the ultimate goal of predicting the nominee. Leveraging various data sources will enable us to understand what may influence the result. \n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/stat10/DS10-Case-Studies/master/Images/dems_heads.jpeg)\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/stat10/DS10-Case-Studies/master/Images/xkcd_comic.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "The comic refers to the fact that Nate Silver's statistical model (which is based mostly on combining information from pre-election polls) correctly predicted the outcome of the 2012 presidential race in all 50 states.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_RrZ_iAWe_h"
   },
   "source": [
    "## DISCUSSION 1\n",
    "\n",
    "In the past, the outcome of political campaigns was predicted by political analysts and pundits, using a combination of their experience, intuition, and personal biases and preferences. In recent decades there has been a shift to a more scientific approach, in which election results are predicted statistically using a poll. A small random sample of voters is asked how they will vote, and from that the result of the entire election is extrapolated. \n",
    "\n",
    "\n",
    "\n",
    "Discuss what factors could have lead to this shift to a more scienfitic approach in determining the outcome of political campaigns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UJHd0b2Wew_"
   },
   "source": [
    "## DISCUSSION 2\n",
    "In practice, a poll's prediction must be taken with a grain of salt, because the sample is only approximately representative of the voting population. For example, in late October 2012, the Gallup poll consistently gave Romney a 6-percentage-point lead in the popular vote, but in fact Obama won the popular vote by 2.6 percentage points. On the other hand, RAND Corporation was biased toward the Democrats and tended to overstate Obama's lead by 1.5 percentage points. \n",
    "\n",
    "**What could lead to a sample (in terms of polls) not representing the population?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5a3va7VWetd"
   },
   "source": [
    "## DISCUSSION 3\n",
    "One approach to overcome this bias is by averaging together the different polls. This is better than trusting any one of them, but it is still rather crude. What if most of them are biased? \n",
    "\n",
    "Silver's approach is very sophisticated, but its key idea is to combine different polls using a weighted average. In a normal average, each data point contributes equally to the result. In a weighted average, some data points contribute more than others. Silver examined how well each polling organization had predicted previous elections, and then weighted their polls according to their accuracy: more biased pollsters had less effect on the weighted average.\n",
    "\n",
    "**Can you think of some things to consider when determining the weight of a poll (i.e. how accurate/reliable is this poll)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZMFVURo7rrUm"
   },
   "source": [
    "#Bias in Survey Sampling\n",
    "\n",
    "In survey sampling, bias refers to the tendency of a sample statistic to systematically over- or under-estimate a population parameter. \n",
    "\n",
    "\n",
    "###Bias Due to Unrepresentative Samples\n",
    "\n",
    "\n",
    "- Undercoverage\n",
    "\n",
    "\n",
    "- Nonresponse bias. \n",
    "\n",
    "\n",
    "- Voluntary response bias.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x98a2dAWrWcz"
   },
   "source": [
    "## DISCUSSION 4\n",
    "\n",
    "**What methods could potentially overcome selection bias?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-F8hUsnrKJv"
   },
   "source": [
    "## Bias Due to Measurement Error\n",
    "\n",
    "A poor measurement process can also lead to bias. In survey research, the measurement process includes the environment in which the survey is conducted,the way that questions are asked, and the state of the survey respondent.\n",
    "\n",
    "## DISCUSSION 5 \n",
    "**Can you give some examples of response bias?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrRfxlZqtQ4j"
   },
   "source": [
    "##Test Your Understanding\n",
    "\n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "\n",
    "\n",
    "1. Random sampling is a good way to reduce response bias. \n",
    "2. To guard against bias from undercoverage, use a convenience sample.\n",
    "3. Increasing the sample size tends to reduce survey bias. \n",
    "4. To guard against nonresponse bias, use a mail-in survey.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* (A) 1 only \n",
    "* (B) 2 only \n",
    "* (C) 3 only \n",
    "* (D) 4 only \n",
    "* (E) None of the above.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4nipPFTqvFyU"
   },
   "source": [
    "##Now to the fun stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "3oGAKxMuNoUH"
   },
   "outputs": [],
   "source": [
    "#@title Load modules\n",
    "## Load your libraries\n",
    "## Libraries, also called \"modules\", contain functions and methods already written \n",
    "## by someone else. All you have to do is pass your argument(s) and call them yourself!\n",
    "\n",
    "\n",
    "## Load the pandas module and give it the name 'pd'.\n",
    "## Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
    "import pandas as pd \n",
    "\n",
    "## Load the python plotting modules\n",
    "## We don't need to import the entire matplotlib. We can just import the plotting utilities.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##Load the numpy module. Numpy is \n",
    "import numpy as np\n",
    "\n",
    "## Load the datetime module\n",
    "import datetime\n",
    "\n",
    "##We might load other modules along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2wBrujdGNyyB"
   },
   "outputs": [],
   "source": [
    "#@title Fetch Polling Data\n",
    "##Now we'll fetch our polling data from FiveThirtyEight. The data for this case study is hosted at the following link. \n",
    "##Pandas has a read_csv function which takes the data_link as an argument.\n",
    "\n",
    "data_link = \"https://projects.fivethirtyeight.com/polls-page/president_primary_polls.csv\"\n",
    "\n",
    "poll_df = pd.read_csv(data_link)\n",
    "\n",
    "##Display the first 10 rows.\n",
    "poll_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OuJzODHJRted"
   },
   "outputs": [],
   "source": [
    "## Notice that the poll_id is repeated several times.\n",
    "## What does each row represent?\n",
    "len(poll_df['poll_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "JMzOyatcRtky"
   },
   "outputs": [],
   "source": [
    "#@title What features/metrics are available in this dataset? Let's take a look at the columns. \n",
    "## What features/metrics are available in this dataset? Let's take a look at the columns. \n",
    "list(poll_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XsHB_9WHSREF"
   },
   "source": [
    "## Discussion 6:\n",
    "\n",
    " >  ### Which metrics can provide us with the most useful insight?\n",
    " >  ### If you had to choose only one, which would it be? \n",
    " >  ### Do any of the columns above look unusual?\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "c-DhfIYgRtyq"
   },
   "outputs": [],
   "source": [
    "#@title Name of the Polls\n",
    "##Print out the names of the polls.\n",
    "\n",
    "list_polls = list(poll_df['pollster'].unique())\n",
    "\n",
    "print(list_polls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "pUMD_ZiZR1o4"
   },
   "outputs": [],
   "source": [
    "#@title Display the unique values from some columns.\n",
    "## Display the unique values in each of the following columns:\n",
    "\n",
    "## fte_grade\n",
    "## methodology\n",
    "## office_type\n",
    "## stage\n",
    "\n",
    "\n",
    "for column_name in ['fte_grade','methodology','office_type','stage']:\n",
    "  list_returned = list(poll_df[column_name].unique())\n",
    "  print(\"Column Name: \", column_name)\n",
    "  print(list_returned)\n",
    "  print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c17HEmHmSov7"
   },
   "source": [
    "## Discussion 7 \n",
    "PCT is the percentage of participants that chose that answer for a given question and poll id. \n",
    "\n",
    "**What should pct sum up to? Why?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MktFynU6PcHe"
   },
   "source": [
    "\n",
    "\n",
    "### What are our main columns of interest? \n",
    "\n",
    "1. Candidate name\n",
    "2. PCT\n",
    "3. Pollster Rating\n",
    "\n",
    "Let's begin by doing some initial visualization to get a feel for the data. Our goal is to create a function that takes a poll id and graphs the pct for each candidate.\n",
    "\n",
    "We'll do this step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "f3pXt-H_Qxdy"
   },
   "outputs": [],
   "source": [
    "#@title Code hidden.\n",
    "## Now let's take everything we learned and put it into a function that\n",
    "## we can call instead of having to replicate those same lines of code everytime\n",
    "## we'd like to graph the polling percentage.\n",
    "import pdb\n",
    "\n",
    "def graph_poll_id(argument_poll_id):\n",
    "  #pdb.set_trace()\n",
    "  #Only get the rows where the column poll_id matches the poll_id passed in as an argument.\n",
    "  series_poll = poll_df[poll_df[\"poll_id\"]==argument_poll_id]\n",
    "  \n",
    "  #Sort\n",
    "  sorted_series_poll = series_poll.sort_values(by=\"pct\")\n",
    "\n",
    "  \n",
    "  ## Second we will remove candidates who have zero percent from the graph.\n",
    "  ## We'll do this by filtering the data such that we only consider rows where\n",
    "  ## the percentage is greater than 0.\n",
    "\n",
    "  ## Let's also give each candidate a different color.\n",
    "  filtered_series_poll = sorted_series_poll[sorted_series_poll['pct'] > 0]\n",
    "\n",
    "  bars = filtered_series_poll['candidate_name']\n",
    "  y_pos = np.arange(len(bars))\n",
    "  performance = filtered_series_poll['pct'] \n",
    "\n",
    "  ## try changing the orders of the colors yourself!\n",
    "  \n",
    "  colors = 'rgbkymc'  #red, green, blue, black, etc.\n",
    "  plt.barh(y_pos, performance, color=colors,align='center', alpha=0.5)\n",
    "  plt.yticks(y_pos, bars)\n",
    "  plt.xlabel('PCT')\n",
    "  \n",
    "  \n",
    "  ## Third we will we will add some metadata (poll name, sample size, date) to give more context.\n",
    "  date_of_poll = list(filtered_series_poll['start_date'])[0]\n",
    "  poll_name = list(filtered_series_poll['pollster'])[0]\n",
    "  \n",
    "  sample_size =  list(filtered_series_poll['sample_size'])[0]\n",
    "  \n",
    "  title_of_graph = \"Polling Percentage \\n Poll Name: {} \\n Date of Poll: {} \\n Sample Size: {}\\n \".format(poll_name,date_of_poll,sample_size)\n",
    "  plt.title(title_of_graph)\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "TPNkjwoBQxnY"
   },
   "outputs": [],
   "source": [
    "#@title Show results for 5 A polls.\n",
    "\n",
    "list_A_minus_poll_ids = list(poll_df[poll_df['fte_grade'] == \"A-\"][\"poll_id\"].unique())\n",
    "\n",
    "print(list_A_minus_poll_ids)\n",
    "\n",
    "## To get the first 5 elements in a list: use splicing list[:5]\n",
    "for poll_id in list_A_minus_poll_ids[:5]:\n",
    "  graph_poll_id(poll_id)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_YCRtRtRLfm"
   },
   "source": [
    "## Reminder: Polls are approximate! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcSOVakFRb54"
   },
   "source": [
    "## First, let's look at who the candidates are more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "nwflCmnnRYLH"
   },
   "outputs": [],
   "source": [
    "#@title Who are the candidates? How many candidates are there?\n",
    "## Who are the candidates? \n",
    "candidates_object = poll_df[\"candidate_name\"].unique()\n",
    "print(candidates_object)\n",
    "\n",
    "## How many candidates are there? \n",
    "len(candidates_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Mg9rO4CTzll"
   },
   "source": [
    "## DISCUSSION 8\n",
    "\n",
    "###1.   Any unusual names there?\n",
    "###2.   Any names you haven't heard of?\n",
    "###3.   Any names that dropped out?\n",
    "###4.   Any names that do not belong to the Democratic party?\n",
    "\n",
    "Since the group of those that are not running is much larger than those that are still running, we will create a list of those candidates that are still running.\n",
    "We will only consider those candidates. \n",
    "\n",
    "As of March 6, 2020, there are only two candidates left in the primary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "qB7n36zqUjtN"
   },
   "outputs": [],
   "source": [
    "#@title Candidates still left in the primary\n",
    "## \n",
    "list_candidates = list(candidates_object[:2])\n",
    "print(list_candidates)\n",
    "print(len(list_candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB-V6RxzZ8MR"
   },
   "source": [
    "# Let's get a polling average.\n",
    "\n",
    "Since there's a lot of polling data, let's focus on the polls from the first half of 2019. This means we will only consider polls conducted between Jan. 1 and June 30, 2019. \n",
    "\n",
    "Our goal is to match the following numbers [in this 538 chart](https://fivethirtyeight.com/features/what-weve-gleaned-about-the-democratic-primary-from-6-months-of-polls/). See the chart with the title: \"How the 2020 Democratic primary field looks six months in.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "f1Pi-oVzbvgy"
   },
   "outputs": [],
   "source": [
    "#@title Get Polling Average from November 1, 2019 to March 5, 2020\n",
    "## First look at the columns we'll use to filter the data. \n",
    "## Columns: 'start_date' and 'end_date'\n",
    "#print(poll_df['start_date'])\n",
    "\n",
    "#print(\"-------------------this is just to seperate the two print statements----------------\")\n",
    "\n",
    "## DIY: Print the end_date column \n",
    "## Remove next line\n",
    "#print(poll_df['end_date'])\n",
    "## Now for us to be able to do manipulations (comparisons) on this data\n",
    "## we need to format it first. We can use pandas to change the formatting.\n",
    "## The returned value of pd.datetime is a timestamp object for comparisons\n",
    "## i.e. its not just a string. \n",
    "\n",
    "## Create a new column called formatted_start_date use the format m/d/y\n",
    "\n",
    "poll_df['formatted_start_date'] = pd.to_datetime(poll_df['start_date'], format='%m/%d/%y')\n",
    "\n",
    "##DIY: create the formatted end date column\n",
    "## Remove next line\n",
    "poll_df['formatted_end_date'] = pd.to_datetime(poll_df['end_date'], format='%m/%d/%y')\n",
    "## See the difference? \n",
    "poll_df['formatted_start_date'] \n",
    "## Now our conditions are that the poll's start date must be \n",
    "## between 1/1/2019 and 6/30/2019. \n",
    "\n",
    "## Let's create a january 1st timestamp object with the same format\n",
    "## as \"formatted_start_date\" column.\n",
    "\n",
    "#jan_1st_date = pd.to_datetime(\"1/1/19\", format='%m/%d/%y')\n",
    "jan_1st_date = pd.to_datetime(\"11/1/19\", format='%m/%d/%y')\n",
    "#print(jan_1st_date)\n",
    "#print(type(jan_1st_date))\n",
    "## DIY: Create the june 30, 2019 timestamp same as above. \n",
    "## Remove next line\n",
    "#june_30th_date = pd.to_datetime(\"6/30/19\", format='%m/%d/%y')\n",
    "june_30th_date = pd.to_datetime(\"3/5/20\", format='%m/%d/%y')\n",
    "#print(june_30th_date)\n",
    "\n",
    "## Now we will create boolean conditions to filter our pandas dataframe\n",
    "\n",
    "#Anytime on or after 1/1/19\n",
    "condition_1 = (poll_df['formatted_start_date'] >= jan_1st_date)\n",
    "\n",
    "## DIY: Create the condition for anytime on or before 6/30/19\n",
    "# Remove next line\n",
    "condition_2 = (poll_df['formatted_start_date'] <= june_30th_date)\n",
    "\n",
    "#Only consider democratic candidates!\n",
    "condition_3 = (poll_df['party'] == \"DEM\")\n",
    "\n",
    "#Only consider those candidates that are still running.\n",
    "condition_4 = (poll_df['candidate_name'].isin(list_candidates))\n",
    "\n",
    "# Apply the conditions! F for filtered.\n",
    "f_poll_df = poll_df[condition_1 & condition_2 & condition_3 & condition_4]\n",
    "\n",
    "## print out the filtered data frame\n",
    "#print(f_poll_df)\n",
    "\n",
    "## Now we will only look at the two important columns: candidate_name and pct\n",
    "f_poll_df = f_poll_df[['candidate_name','pct']]\n",
    "\n",
    "## print it out now. which columns are showing?\n",
    "#print(f_poll_df)\n",
    "\n",
    "## Now group by candidate name\n",
    "grouped_poll_df = f_poll_df.groupby(\"candidate_name\")\n",
    "\n",
    "## For each group (candidate), take the mean.\n",
    "mean_groups = grouped_poll_df.mean()\n",
    "\n",
    "## Then sort the values by pct. \n",
    "mean_groups.sort_values(by=\"pct\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqbucFBAgxu6"
   },
   "source": [
    "## What can we say now?\n",
    "\n",
    "We just added another layer of information by \n",
    "1. using the PCT\n",
    "2. averaging the PCTs\n",
    "\n",
    "Can we make our prediction even better? What factors can we further consider that would provide us with more information?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-m2ehThtkFUN"
   },
   "source": [
    "## What about a weighted average? \n",
    "\n",
    "\n",
    "### What is a weighted average?\n",
    "\n",
    "A weighted average or mean is one where each item being averaged is multiplied by a number (weight) based on the item's relative importance, rather than treating each item equally. The weights or weightings are the equivalent of having that many similar items with the same value involved in the average. The result is summed and the total is divided by the sum of the weights.\n",
    "\n",
    "#### What is the weight in an arithmetic average? \n",
    "An arithmetic average can be considered a special case where all values are valued (weighted) equally.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfDUB1mBWrsv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TiJLAOvXWsJA"
   },
   "source": [
    "## Discussion 9\n",
    "Give an everyday example of a weighted average in real life. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeIW8HWq3z4h"
   },
   "source": [
    "##Which weighting scheme will we use? \n",
    "\n",
    "Older data contains more random noise than recent data: You not only have the random noise from sampling error, but also from the changes in polls that build up over time. By reducing the weight of this less reliable data, we diminish that noise.\n",
    "\n",
    "If we keep reducing the weight of old data, though, we eventually hit a point where we start throwing out more useful information than random noise, and going any further starts to make our estimate worse. \n",
    "\n",
    "Weighting depends partially on how much the polls changes from day to day, but it also depends on how much you can tell about the underlying trends from your data. In general, the more your recent observations tell you about the underlying candidate favorability, or the more data you have, the more aggressively you can discount past results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZeQ-I24xgLw"
   },
   "outputs": [],
   "source": [
    "\n",
    "f_poll_df = poll_df[condition_1 & condition_2 & condition_3 & condition_4]\n",
    "\n",
    "f_poll_df['step'] = june_30th_date - f_poll_df['formatted_start_date']\n",
    "\n",
    "\n",
    "f_poll_df = f_poll_df[['candidate_name','formatted_start_date','pct','step']]\n",
    "f_poll_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PFc6RUyFyhC"
   },
   "source": [
    "## Exponentially Decaying Weights \n",
    "\n",
    "\n",
    "We will use exponentially decaying weights to weight more recent polls heavier than older polls. We will use this exponential decay function: α(1−α)^t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "n8z-iDMxJ7L9"
   },
   "outputs": [],
   "source": [
    "#@title Calculate Weighted Average\n",
    "\n",
    "#@title Define get weight function\n",
    "#We will use this exponential decay function: α(1−α)^t\n",
    "## source #https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc431.htm\n",
    "import math\n",
    "def get_weight(a,t):\n",
    "  base = math.fabs(1-a)\n",
    "  val = a*math.pow(base,t)\n",
    "  #print(\"a: {}, t: {} , val:{}\".format(a,t,val))\n",
    "  return val\n",
    "\n",
    "\n",
    "# see https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc431.htm\n",
    "sp = (june_30th_date - f_poll_df['formatted_start_date'])\n",
    "##To use the function above, we need to calculate the time change, or delta, which will be the \n",
    "##step size, or t. \n",
    "deltas = sp.apply(lambda x: x.days)\n",
    "\n",
    "##Try using t as is, and try using t/7 (a week of information, sort of normalization\n",
    "##How do your results change?\n",
    "weights = deltas.apply(lambda x: get_weight(a=1.9,t=x/7))\n",
    "f_poll_df['weights'] = weights\n",
    "f_poll_df['weights'] \n",
    "##Group by candidate and get the mean pct. Then sort the values.\n",
    "\n",
    "grouped = f_poll_df.groupby(\"candidate_name\")\n",
    "\n",
    "grouped.mean().sort_values(by=\"pct\",ascending=False)['pct']\n",
    "\n",
    "##Now let's calculate a weighted average. \n",
    "\n",
    "weighted_averages = {}\n",
    "\n",
    "\n",
    "for candidate in grouped.groups:\n",
    "  \n",
    "  group = grouped.get_group(candidate)\n",
    "  \n",
    "  weights_list = []\n",
    "  pct_list = []\n",
    "  for row_number in range(len(group)):\n",
    "    row = group.values[row_number]\n",
    "    \n",
    "    pct = row[2]\n",
    "    weight = row[4]\n",
    "    weights_list.append(weight)\n",
    "    pct_list.append(pct)\n",
    "    \n",
    "  \n",
    "  \n",
    "  weighted_average = np.average(a=pct_list,weights=weights_list)\n",
    " \n",
    "  ## Save the results in a dictionary for displaying later\n",
    "  weighted_averages[candidate] = weighted_average\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Sort the results we got above, then print them out.\n",
    "sorted_w_avgs = sorted(weighted_averages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#for candidate, weighted_avg in sorted_w_avgs:\n",
    "#  print(candidate, weighted_avg)\n",
    "\n",
    "## Here they are again side by side:\n",
    "\n",
    "unweighted_averages = grouped.mean().sort_values(by=\"pct\",ascending=False)['pct']\n",
    "\n",
    "weighted_averages_series = pd.Series(dict(sorted_w_avgs))\n",
    "\n",
    "combined_df = pd.DataFrame(weighted_averages_series)\n",
    "combined_df.columns = [\"weighted_average\"]\n",
    "combined_df['unweighted_average'] = unweighted_averages\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a_piWSLVl-y"
   },
   "source": [
    "\n",
    "## Next we will do a fun vizualization using a map of the United States.\n",
    "\n",
    "Oftentimes, it's easier to vizualize certain data using more than just a graph. For example, our polling data includes geographic information (state in which the participants were polled). \n",
    "\n",
    "One way to incorporate this information is by using a chloropleth. A chloropleth map is a map in which areas are shaded or patterned in proportion to the measurement of the statiscial variable being displayed on the map, such as population density or per-capita income.\n",
    "\n",
    "Choropleth maps provide an easy way to visualize how a measurement varies across a geographic area or show the level of variability within a region.\n",
    "\n",
    "##We will use a chloropleth map of the USA to vizualize the leader in each state (those that we have data for).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "F6NgrxUjWDtB"
   },
   "outputs": [],
   "source": [
    "#@title Map of Each State's (Weighted Avg) Poll Leader\n",
    "## Try grouping by state on this same filtered data.\n",
    "\n",
    "## SOLUTION (will be hidden/removed)\n",
    "\n",
    "filtered_poll_df = poll_df[condition_1 & condition_2 & condition_3][['state','candidate_name','pct']]\n",
    "\n",
    "grouped_state_poll_df = filtered_poll_df.groupby([\"state\",'candidate_name'])\n",
    "\n",
    "\n",
    "mean_state_df = grouped_state_poll_df.mean().sort_values(by=\"pct\",ascending=False)\n",
    "\n",
    "\n",
    "##Let's print Bernie's average in Alabama.\n",
    "#print(\"Bernie's average in Alabama\", mean_state_df['pct']['Alabama']['Bernard Sanders'])\n",
    "\n",
    "## Your turn! Print Elizabeth Warren's average in Massachussetts and California.\n",
    "\n",
    "##SOLUTION (will be removed/hidden later)\n",
    "#print(mean_state_df['pct']['California']['Elizabeth Warren'])\n",
    "#print(mean_state_df['pct']['Massachusetts']['Elizabeth Warren'])\n",
    "\n",
    "\n",
    "dict_state = {}\n",
    "\n",
    "list_means = []\n",
    "for name, group in grouped_state_poll_df:\n",
    "  \n",
    "  \n",
    "    state,candidate = name\n",
    "    data = (candidate, group.mean()[0])\n",
    "    if state not in dict_state.keys():\n",
    "      dict_state[state] = [data]\n",
    "    else:\n",
    "      dict_state[state].append(data)\n",
    "\n",
    "\n",
    "\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Palau': 'PW',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "}\n",
    "## Let's get the leader in each state so we can plot it!\n",
    "list_of_rows = []\n",
    "data_max_states = {}\n",
    "\n",
    "for state in dict_state.keys():\n",
    "  list_state_means = dict_state[state]\n",
    "  max_for_state = max(list_state_means, key=lambda x:x[1])\n",
    "  candidate, pct = max_for_state\n",
    "  state_abbrev = us_state_abbrev[state]\n",
    "  \n",
    "  print(state_abbrev,candidate, pct)\n",
    "  row = [state_abbrev,candidate, pct]\n",
    "  list_of_rows.append(row)\n",
    "  #data_max_states[state] = max_for_state\n",
    "\n",
    "df_new = pd.DataFrame(list_of_rows,columns=[\"State\",\"Candidate\",\"PCT\"])\n",
    "\n",
    "\n",
    "\n",
    "#We will need to install Plotly first on our virtual environments. Use !pip install plotly\n",
    "!pip install plotly==4.1.0\n",
    "\n",
    "##Then load the module.\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "##Create the figure object\n",
    "fig = go.Figure(data=go.Choropleth(\n",
    "    locations=df_new['State'], # Spatial coordinates\n",
    "    z = df_new['PCT'].astype(float), # Data to be color-coded\n",
    "    text = df_new[\"Candidate\"],\n",
    "    locationmode = 'USA-states', # set of locations match entries in `locations`\n",
    "    colorscale = 'Blues',\n",
    "    #mode=\"text\",\n",
    "    colorbar_title = \"PCT Percentage\",\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Leaders in Polls',\n",
    "    geo_scope='usa', # limite map scope to USA\n",
    ")\n",
    "\n",
    "\n",
    "##Show the figure. You can hover and zoom!\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foLzSmyOYKOL"
   },
   "source": [
    "# PART 2: DEBATE ANALYSIS \n",
    "\n",
    "##In this section we will see how the Democratic debates afftected the polling averages! \n",
    "\n",
    "#18.1 million ...\n",
    "people watched the second round of the Democratic debate across NBC, MSNBC, and Telemundo, The Los Angeles Times reported, making it the MOST WATCHED Democratic debate in television history — and even more people tuned in to online livestreams. The previous viewership record for a Democratic debate took place in October 2015, when five Democratic candidates faced off as 15.4 million people watched. On Thursday night, there were twice as many candidates, and more viewers than ever before. [source](https://www.bustle.com/p/how-many-people-watched-the-democratic-debate-night-two-set-a-record-18157398)\n",
    "\n",
    "![](https://raw.githubusercontent.com/stat10/DS10-Case-Studies/master/Images/dem_debate_warren_booker.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "The goal for this section is to use what we learned above to calculate a weighted average before and after the debate. Then we will calculate the net change between the those time periods to see how the debate might have impacted a candidates favorability in the polls. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJK4_roBj-kr"
   },
   "source": [
    "### For this section we will use FiveThirtyEight's pre-calculated weights for our weighted average. In addition, we will only consider a certain set of polls. \n",
    "\n",
    "According to Nate Silver:\n",
    "\n",
    ">\"So far, there have been five national polls conducted entirely after the debate that allow for a direct comparison to an earlier poll by the same pollster. These are the polls from Quinnipiac University, Ipsos, Morning Consult,1 YouGov and HarrisX.2 \n",
    "\n",
    "> I weighted the polls based on sample size and the pollster’s rating, as we do for our Trump approval tracker and in our election models.\"\n",
    "\n",
    "See below for the polls and their associated weights.\n",
    "\n",
    "\n",
    "Polls included in the weighted average are Quinnipiac (weight 1.40), Ipsos (1.49),  Morning Consult (2.13), \n",
    "HarrisX (1.16) and YouGov (0.94).  Only candidates who participated in the debates are listed in the table.\n",
    "\n",
    "Note: The second debate occured on July 30 and 31st.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "woD533T2fDva"
   },
   "outputs": [],
   "source": [
    "#@title Pre (1 week) and Post (1 week) Debate (2/25/2020) Analysis\n",
    "weights_dict = {\n",
    "    'Quinnipiac University': 1.40,\n",
    "    \"Ipsos\": 1.49,\n",
    "    \"Morning Consult\": 2.13,\n",
    "    'HarrisX': 1.16,\n",
    "    \"YouGov\": 0.94\n",
    "  \n",
    "}\n",
    "\n",
    "candidates_participated = ['Joseph R. Biden Jr.',\n",
    " 'Bernard Sanders',\n",
    " 'Elizabeth Warren',\n",
    " '#Kamala D. Harris',\n",
    " 'Pete Buttigieg',\n",
    " #\"Beto O'Rourke\",\n",
    " #'Cory A. Booker',\n",
    " #'Julián Castro',\n",
    " 'Amy Klobuchar']\n",
    " #'Kirsten E. Gillibrand',\n",
    " #'John K. Delaney',\n",
    " #'Tulsi Gabbard',\n",
    " #'John Hickenlooper',\n",
    " #'Jay Robert Inslee',\n",
    " #'Marianne Williamson',\n",
    " #'Andrew Yang',\n",
    " #'Wayne Messam', #didn't make it.\n",
    " #'Seth Moulton',#didn't make it.\n",
    " #'Tim Ryan',\n",
    " #'Michael F. Bennet',\n",
    " #'Steve Bullock',\n",
    " #'Bill de Blasio'\n",
    " #'Joe Sestak',#didn't make it.\n",
    " #'Tom Steyer'#didn't make it.\n",
    "                  #  ]\n",
    "\n",
    "## Remind ourselves of all the pollsters in our original dataset.\n",
    "#poll_df['pollster'].unique()\n",
    "\n",
    "\n",
    "\n",
    "##We'll reuse the code above for a more precise investigation of the pcts.\n",
    "\n",
    "def get_weighted_averages(f_poll_df):\n",
    "  f_poll_df = f_poll_df[['candidate_name','pct','pollster']]\n",
    "  grouped = f_poll_df.groupby(\"candidate_name\")\n",
    "  weighted_averages = {}\n",
    "\n",
    "\n",
    "  for candidate in grouped.groups:\n",
    "   \n",
    "    group = grouped.get_group(candidate)\n",
    "    #print(candidate,group)\n",
    "    weights_list = []\n",
    "    pct_list = []\n",
    "    for row_number in range(len(group)):\n",
    "      row = group.values[row_number]\n",
    "      \n",
    "      pct = row[1]\n",
    "      pollster = row[2]\n",
    "      weight = weights_dict[pollster]\n",
    "      weights_list.append(weight)\n",
    "      pct_list.append(pct)\n",
    "\n",
    "\n",
    "\n",
    "    weighted_average = np.average(a=pct_list,weights=weights_list)\n",
    "    #np.mean(np.multiply(pct_list,weights_list))\n",
    "    #weighted_average = np.mean(np.multiply(pct_list,weights_list))\n",
    "    \n",
    "\n",
    "    ## Save the results in a dictionary for displaying later\n",
    "    weighted_averages[candidate] = weighted_average\n",
    "\n",
    "\n",
    "  sorted_w_avgs = sorted(weighted_averages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  for candidate, weighted_avg in sorted_w_avgs:\n",
    "    print(candidate, weighted_avg)\n",
    "  return sorted_w_avgs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  #### Recycling and modularizing the code from before. \n",
    "\n",
    "##We'll pass the following as arguments to our function:\n",
    "## - entire dataframe (this contains our polling data)\n",
    "## - debate num (as of now only 2 debates have occured (6/26,27 and 7/30,31))\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def before_after_debate(poll_df,debate_num):\n",
    "  \n",
    "  ## Now we will create boolean conditions to filter our pandas dataframe\n",
    "  if debate_num == 1:\n",
    "    print(\"Will analyze debate ONE \\n\")\n",
    "    night_one = pd.to_datetime(\"6/26/19\", format='%m/%d/%y')\n",
    "    night_two = pd.to_datetime(\"6/27/19\", format='%m/%d/%y')\n",
    "  elif debate_num == 2:\n",
    "    print(\"Will analyze debate TWO \\n\")\n",
    "    night_one = pd.to_datetime(\"7/30/19\", format='%m/%d/%y')\n",
    "    night_two = pd.to_datetime(\"7/31/19\", format='%m/%d/%y')\n",
    "    days_to_subtract = 7\n",
    "    timeframe_before = night_one - timedelta(days=days_to_subtract)\n",
    "    \n",
    "  elif debate_num == 3:\n",
    "    \n",
    "    night_one = pd.to_datetime(\"2/25/20\", format='%m/%d/%y')\n",
    "    night_two = pd.to_datetime(\"2/25/20\", format='%m/%d/%y')\n",
    "    days_to_subtract = 7\n",
    "    timeframe_before = night_one - timedelta(days=days_to_subtract)\n",
    "  \n",
    "  #aug_7_date = pd.to_datetime(\"8/07/19\", format='%m/%d/%y')\n",
    "  \n",
    "  #condition_before = (poll_df['formatted_start_date'] < night_one)\n",
    "  condition_before = (poll_df['formatted_start_date'] < night_one) & (poll_df['formatted_start_date'] > timeframe_before)\n",
    "  \n",
    "  \n",
    "  ##to copy Nate Silver, do only until august 7th\n",
    "  #condition_after = (poll_df['formatted_start_date'] > night_two) & (poll_df['formatted_end_date'] < aug_7_date) \n",
    "  #condition_after = (poll_df['formatted_start_date'] >= night_one) & (poll_df['formatted_end_date'] <= aug_7_date) \n",
    "  condition_after = (poll_df['formatted_start_date'] >= night_one) & (poll_df['formatted_end_date'] <= pd.to_datetime(\"3/1/20\", format='%m/%d/%y')) \n",
    "  \n",
    "  ## The pollster is one of the 5 listed above.\n",
    "  condition_1 = (poll_df['pollster'].isin(weights_dict.keys()))\n",
    " \n",
    "  #Only consider democratic candidates!\n",
    "  condition_2 = (poll_df['party'] == \"DEM\")\n",
    "\n",
    "  #Only consider those candidates that are still running.\n",
    "  condition_3 = (poll_df['candidate_name'].isin(candidates_participated))\n",
    "  print(candidates_participated)\n",
    "  \n",
    "  # Apply the conditions! F for filtered.\n",
    "\n",
    "  common_conditions = (condition_1 & condition_2 & condition_3)\n",
    "\n",
    "  data_before =  poll_df[common_conditions & condition_before]\n",
    "  data_after =  poll_df[common_conditions & condition_after]\n",
    "  \n",
    "  ## Perform some basic checks on our filtered datasets. \n",
    "  ## check the columns \n",
    "  #print(data_before.head(n=5))\n",
    "  #print(data_after.head(n=5))\n",
    "  \n",
    "  ###\n",
    "  #print(data_before['formatted_start_date'].unique())\n",
    "  \n",
    "  ##check the number of rows\n",
    "  #print(len(data_before))\n",
    "  #print(len(data_after))\n",
    "  ##check that we only use the specified pollsters\n",
    "  #print(data_before[\"pollster\"].unique())\n",
    "  #print(data_after[\"pollster\"].unique())\n",
    "  \n",
    "  sorted_before_w_avgs = get_weighted_averages(data_before)\n",
    "  #print(\"-----------\\n\\n----------\") #for seperation \n",
    "  sorted_after_w_avgs = get_weighted_averages(data_after)\n",
    "\n",
    "  return sorted_before_w_avgs, sorted_after_w_avgs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Call our function and save the results so we can use them later!\n",
    "\n",
    "sorted_before_w_avgs, sorted_after_w_avgs = before_after_debate(poll_df, 3)\n",
    "\n",
    "\n",
    "\n",
    "#sorted_before_w_avgs.keys ==sorted_after_w_avgs.keys\n",
    "# we need to create 3 seperate lists for ordering purposes so we cn presnet the data accurately.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(sorted_before_w_avgs)==len(sorted_after_w_avgs))\n",
    "cand_names = []\n",
    "\n",
    "before_pcts = []\n",
    "\n",
    "after_pcts = []\n",
    "\n",
    "net_changes = []\n",
    "\n",
    "for i in range(len(sorted_before_w_avgs)):\n",
    "  tuple_before = sorted_before_w_avgs[i]\n",
    "  cand_b = tuple_before[0]\n",
    "  \n",
    "  pct_before = tuple_before[1]\n",
    "  \n",
    "  ##look for the pct after the debate for that candidate\n",
    "  pct_after = [tuple_a[1] for tuple_a in sorted_after_w_avgs if tuple_a[0] == cand_b][0]\n",
    "  \n",
    "  net_change = pct_after - pct_before\n",
    "  #print(cand_b, pct_before, pct_after, net_change)\n",
    "  cand_names.append(cand_b)\n",
    "  before_pcts.append(pct_before)\n",
    "  after_pcts.append(pct_after)\n",
    "  net_changes.append(net_change)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Load the python module with table generating functions\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#fig = go.Figure(data=[go.Table(header=dict(values=['Candidate Name', 'Before Debate','After Debate','Net Change']),\n",
    "#                 cells=dict(values=[cand_names, before_pcts, after_pcts, net_changes]))\n",
    "#                     ])\n",
    "#fig.show()\n",
    "\n",
    "## Limit the pcts and net changes to 2 decimal points only.\n",
    "\n",
    "##the round function takes as an argument the element to be rounded and the number of digits to round to. \n",
    "## in our case we'd like to keep only 2 decimal points. \n",
    "\n",
    "rounded_before_pcts = [round(element,2) for element in before_pcts]\n",
    "rounded_after_pcts = [round(element,2) for element in after_pcts]\n",
    "rounded_net_changes = [round(element,2) for element in net_changes]\n",
    "\n",
    "##TBD \n",
    "##Could be DIY \n",
    "\n",
    "#Decimal Red, Green, Blue, and Opacity Color Codes\n",
    "#This page demonstrates the decimal representation of color of the form rgba(R,G,B,a), where R, G, and B are the decimal \n",
    "#values for the red, green, and blue values of the color on the range 0 to 255 and a is the opacity of the color (a = 0 = transparent; a = 1 = opaque). \n",
    "\n",
    "#https://www.december.com/html/spec/colorrgbadec.html\n",
    "\n",
    "##Use the link above and try changing the colors and the opacity! \n",
    "##DIY\n",
    "\n",
    "fig = go.Figure(data=[go.Table(header=dict(values=['Candidate Name', 'Before Debate','After Debate','Net Change']),\n",
    "                 cells=dict(values = [cand_names, rounded_before_pcts, rounded_after_pcts, rounded_net_changes],\n",
    "                           fill = dict(color=['rgb(245,245,245)',#unique color for the first column\n",
    "                                              'rgb(245,245,245)',#unique color for the second column\n",
    "                                              'rgb(245,245,245)',#unique color for the third column\n",
    "                                             ['rgba(0,250,0, 0.3)' if val>=0 else 'rgba(250,0,0, 0.3)' for val in rounded_net_changes] \n",
    "                                             \n",
    "                                             \n",
    "                                             ]\n",
    "                               #the cells in the second column colored with green or red, according to their values\n",
    "                                         ))\n",
    "                              \n",
    "                              \n",
    "                              )\n",
    "                     ])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2QpJaRTMef43"
   },
   "source": [
    "## DISCUSSION 10\n",
    "- How did the trends change before and after the democratic debate ? \n",
    "- Are the changes real or random fluctuations/noise? \n",
    "- Can we conclusively say that the net changes are directly due to the debates? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwqBqdtPb1XZ"
   },
   "source": [
    "# PART 3: Linear Regression Model - Primary Winner vs. Trump?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S37WJttrod7d"
   },
   "source": [
    "# **An Analysis of Presidential Approval Ratings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "DTB-WR9DR_rQ"
   },
   "outputs": [],
   "source": [
    "#@title Show Approval Ratings\n",
    "# Midterm election happens at: 365*2 - 79 = 651 days into office\n",
    "\n",
    "# Approval Rating Data\n",
    "# The dates during a presidency when approval ratings are measured:\n",
    "dates = [4,30,60,90,120,150,180,210,240,270,300,330,360,390,420,450,480,510,540,570,600,630,651]\n",
    "\n",
    "# Trump\n",
    "trump_approval_rate = [45.5,44.3,42.6,41.9,38.9,38.7,38.7,37.9,38.8, 38.1,36.5,39.1,41.4, 40.5,40.8,42.3,42.3,41.4,42.1,41.9,40,41.8,42.0]\n",
    "\n",
    "# Obama (lost both house and senate in midterm)\n",
    "obama_approval_rate = [68,62.7,59.2,60.2,60.4,57.9,55.1,52,53.2, 52.6, 52.1, 49.7,49.2,48.1,48.1,47.9,49.4,47.8,47, 45.9, 45.2, 45.2,44.9]\n",
    "\n",
    "# George W Bush\n",
    "gb_approval_rate = [46,52.2,55.7,54.1,54.4,52.2, 52.8,52.3,82.7,88.1,85.8,83.2,81.2,78.3,77.8,73.8,70.3,71.2,69.4,65.8,64.8,62.6,60.1]\n",
    "\n",
    "# Clinton (lost both house and senate in midterm)\n",
    "clinton_approval_rate = [54,53.2,55.4,53.4,47.9,42.9,45.7,44.2,48.3,49.7,46.9,54.5,56.2,55.4,50.9,53.3,54.2,50.8,46,45.3,43.2,43.9,45.4]\n",
    "\n",
    "# George H. W. Bush (lost both house and senate in midterm)\n",
    "ghwb_approval_rate = [61,61,59.1,58,61.9,70.2,67.1,68.8,69.5,67.7,69.8,71.2,78.3,73.3,62.2,68.1,66.8,67.9,63.2,59.8,75.9,66,54.9]\n",
    "\n",
    "# Reagan (lost house, same senate)\n",
    "reagan_approval_rate = [54,54,60,67.6,68,59,58.1,60.1,60.1,55.6,53.3,49.2,48.9,46.9,46.9,45,43.6,45,44,41.1,42,42,42]\n",
    "\n",
    "# Carter\n",
    "carter_approval_rate = [69.8,66,74.9,62.8,64.9,62.9,62,60.7,54.3,58.5,54.5,57,55,52.1,50.1,48,42.1,44,39.9,39,42.1,48.1,49]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Before performing the regressions, we need to \"reshape\" the date data to transpose from being a horizontal list to\n",
    "# a vertical vector of numbers... this is just something that sklearn requires\n",
    "dates = np.array(dates).reshape(-1,1)\n",
    "linear_regressor = LinearRegression()  # create object from the class\n",
    "markers = [\".\", \"^\", \"v\", \"s\", \"8\", \"p\", \"P\"]\n",
    "names = [\"Trump\", \"Obama\", \"G.W. Bush\", \"Clinton\", \"G.H.W. Bush\", \"Reagan\", \"Carter\"]\n",
    "ratings = [trump_approval_rate, \n",
    "          obama_approval_rate, \n",
    "          gb_approval_rate, \n",
    "          clinton_approval_rate, \n",
    "          ghwb_approval_rate, \n",
    "          reagan_approval_rate, \n",
    "          carter_approval_rate]\n",
    "\n",
    "# loop through all the ratings and add a scatter plot + regression line to the plot\n",
    "for i, approval_rate in enumerate(ratings):\n",
    "  plt.scatter(dates, approval_rate, marker=markers[i], label = names[i])\n",
    "  linear_regressor.fit(dates, approval_rate)  # perform linear regression\n",
    "  pred = linear_regressor.predict(dates)  # make predictions\n",
    "  plt.plot(dates, pred)\n",
    "\n",
    "plt.title(\"Approval Ratings\")\n",
    "plt.xlabel(\"Days Since Start of Presidency\")\n",
    "plt.ylabel(\"Approval Rating as Percentage\")\n",
    "plt.legend()\n",
    "plt.gcf().set_size_inches(15, 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "be_tq5qYSVNi"
   },
   "outputs": [],
   "source": [
    "#@title Fit a linear regression to each president. Calculate relative % change in approval from start of presidency to day 420\n",
    "# loop through all ratings and create a separate scatter plot + regression line for each one\n",
    "for i, approval_rate in enumerate(ratings):\n",
    "  plt.scatter(dates, approval_rate)\n",
    "  linear_regressor.fit(dates, approval_rate)  # perform linear regression\n",
    "  pred = linear_regressor.predict(dates)  # make predictions\n",
    "  plt.plot(dates, pred)\n",
    "  \n",
    "  # Calculate the relative % change in approval from start of presidency to day 420\n",
    "  diff = round(int(approval_rate[14] - approval_rate[0])/ int(approval_rate[0]) * 100)\n",
    "  plt.title(\"Approval Rating: {} Changed by {}% From Start to Month 16\".format(names[i], diff))\n",
    "  plt.xlabel(\"Days Since Start of Presidency\")\n",
    "  plt.ylabel(\"Approval Rating as Percentage\")\n",
    "  plt.gcf().set_size_inches(8, 6)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "i2mk6O8CYQIR"
   },
   "outputs": [],
   "source": [
    "#@title # Load Data. You must select approval_topline.csv from your computer once downloaded from https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "uploaded = files.upload() # perhaps easier way to do this in google colab?\n",
    "import io\n",
    "\n",
    "# This will load in the data. You must select approval_topline.csv from your computer once downloaded\n",
    "# from https://projects.fivethirtyeight.com/trump-approval-ratings/\n",
    "approvals = pd.read_csv(io.StringIO(uploaded['approval_topline.csv'].decode('utf-8')))\n",
    "approvals.head() # inspect first few rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "o16jqVPha9Fx"
   },
   "outputs": [],
   "source": [
    "#@title Graph a regression of Donald Trumps approval rating (using all polls as data points)\n",
    "# convert the dates from strings to pandas DateTime objects\n",
    "approvals[\"modeldate\"] = pd.to_datetime(approvals[\"modeldate\"], infer_datetime_format=True)  \n",
    "\n",
    "# this line converts all the DateTime objects to TimeDeltas since we would like every date to be\n",
    "# the time elapsed since Trump first took office (the first poll date will suffice for the start of his presidency)\n",
    "approvals[\"modeldate\"] = (approvals[\"modeldate\"] - approvals[\"modeldate\"].iloc[-1])\n",
    "\n",
    "# now we will only use the \"days\" portion of each TimeDelta\n",
    "approvals[\"modeldate\"] = approvals[\"modeldate\"].map(lambda x: x.days)\n",
    "\n",
    "# our dates now look like this! They are of type int\n",
    "#approvals[\"modeldate\"].head(10)\n",
    "\n",
    "# Graph a regression of Donald Trumps approval rating (using all polls as data points)\n",
    "# Also plot the result of every poll\n",
    "dates = np.array(approvals[\"modeldate\"]).reshape(-1,1)\n",
    "\n",
    "plt.scatter(dates, approvals[\"approve_estimate\"])\n",
    "linear_regressor.fit(dates, approvals[\"approve_estimate\"])  # perform linear regression\n",
    "pred = linear_regressor.predict(dates)  # make predictions\n",
    "plt.plot(dates, pred, \"red\")\n",
    "\n",
    "plt.title(\"Donald Trump Approval Rating\")\n",
    "plt.xlabel(\"Days Since Start of Presidency\")\n",
    "plt.ylabel(\"Approval Rating as Percentage\")\n",
    "plt.gcf().set_size_inches(15, 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "pu0uaKDyfp7M"
   },
   "outputs": [],
   "source": [
    "#@title Show OLS Results\n",
    "# Redo the above but using a different package to perfrom regression\n",
    "# this package will give us a nice output similar to what's given in R\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "mod = ols(formula = \"approve_estimate ~ modeldate\", data=approvals)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZCr_ySKmK4g"
   },
   "outputs": [],
   "source": [
    "# We see that the intercept is nearls 40 and there is a slight positive slope (0.0021).\n",
    "# The slope has a standard error of 9.85e-05 and it's p-value is extremely small, which indicates\n",
    "# that the probability of observing this positve slope given that the true slope is 0 (null hypothesis)\n",
    "# is very unlikely.\n",
    "\n",
    "# However, we also see from the data that a linear model may not be the best fit for this data! It seems\n",
    "# Trump's approval rating sank throughout the first 300 days of his presidency but now it is on a slow rise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwPENhKGfucP"
   },
   "source": [
    "## DISCUSSION 11\n",
    "- What assumptions are made under this analysis? \n",
    "- What assumpotions would you challenge? \n",
    "- What would you do differently?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "QqbucFBAgxu6",
    "-m2ehThtkFUN"
   ],
   "name": "REDUCED_Democratic_Primary_Case_Study_3_6_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
